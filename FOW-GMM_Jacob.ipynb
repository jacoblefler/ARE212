{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've discussed three different approaches to calculating the optimal weighting matrix in GMM problems, and developed a Monte Carlo Framework in a Jupyter notebook to explore the finite sample properties of one of these (the two-step estimator) in the linear IV case.   This discussion involves a comparison of these three different approaches.\n",
    "\n",
    "1. Using the notation developed in lecture and in the notebook, what can we say about the finite sample performance of the two-step, iterated, and continuously updated GMM estimators when $k=\\ell$?  \n",
    "2. Using the framework of the Jupyter notebook, what can you say about the behavior of the three estimators?  Fix a given $(N,\\ell,\\mbox{truth})$ and compare the bias and efficiency of these estimators.\n",
    "3. Choose a parameter to vary (of the parameters given in the previous question) and explore how the behavior of the different estimators changes with that parameter (e.g., at what rate do the estimators converge as $N\\rightarrow\\infty$).  \n",
    "4. Sometimes we're willing to assume prior knowledge about the error structure of a given problem which we can exploit in estimating our weighting matrix.  For example, the Monte Carlo DGP in the notebook has homoskedastic $u$, but this is not something we assumed we knew in constructing the optimal weighting matrix.  If we exploited this prior knowledge, how could this change our estimator for the weighting matrix?\n",
    "5. Write an alternative function for estimating the optimal weighting matrix in the homoskedastic case.  How does this affect the empirical distribution of the estimates in the Monte Carlo experiment?\n",
    "6. What if we had heteroskedasticity of the form $\\mbox{E}(u_j^2|Z_j) = (Z_j^\\top Z_j)\\sigma^2$ for all observations $j$ (note that here the $Z_j$ is taken to be a column vector)?   Is this consistent with IV assumptions?   What would be the form of the optimal weighting matrix?  How might you implement an estimator of the weighting matrix that exploited this knowledge?  Can you think of a simple case in which this might be a reasonable model of the error structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by shamelessly stealing Ethan's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73115726]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "## Play with us!\n",
    "beta = 1     # \"Coefficient of interest\"\n",
    "gamma = 1    # Governs effect of u on X\n",
    "sigma_u = 1  # Note assumption of homoskedasticity\n",
    "## Play with us!\n",
    "\n",
    "# Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ\n",
    "\n",
    "ell = 4 # Play with me too!\n",
    "\n",
    "# Arbitrary (but deterministic) choice for VXZ\n",
    "A = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1)) \n",
    "\n",
    "\n",
    "## Below here we're less playful.\n",
    "\n",
    "# Var([X,Z]|u) is constructed so that pos. def.\n",
    "VXZ = A.T@A \n",
    "\n",
    "Q = VXZ[1:,[0]]  # EZX'\n",
    "\n",
    "truth = (beta,gamma,sigma_u,VXZ)\n",
    "\n",
    "## But play with Omega if you want to introduce heteroskedascity\n",
    "Omega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')\n",
    "\n",
    "# Asymptotic variance of optimally weighted GMM estimator:\n",
    "print(inv(Q.T@inv(Omega)@Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import distributions as iid\n",
    "\n",
    "def dgp(N,beta,gamma,sigma_u,VXZ):\n",
    "    \"\"\"Generate a tuple of (y,X,Z).\n",
    "\n",
    "    Satisfies model:\n",
    "        y = X@beta + u\n",
    "        E Z'u = 0\n",
    "        Var(u) = sigma^2\n",
    "        Cov(X,u) = gamma*sigma_u^2\n",
    "        Var([X,Z}|u) = VXZ\n",
    "        u,X,Z mean zero, Gaussian\n",
    "\n",
    "    Each element of the tuple is an array of N observations.\n",
    "\n",
    "    Inputs include\n",
    "    - beta :: the coefficient of interest\n",
    "    - gamma :: linear effect of disturbance on X\n",
    "    - sigma_u :: Variance of disturbance\n",
    "    - VXZ :: Cov([X,Z|u])\n",
    "    \"\"\"\n",
    "    \n",
    "    u = iid.norm.rvs(size=(N,1))*sigma_u\n",
    "\n",
    "    # \"Square root\" of VXZ via eigendecomposition\n",
    "    lbda,v = np.linalg.eig(VXZ)\n",
    "    SXZ = v@np.diag(np.sqrt(lbda))\n",
    "\n",
    "    # Generate normal random variates [X*,Z]\n",
    "    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n",
    "\n",
    "    # But X is endogenous...\n",
    "    X = XZ[:,[0]] + gamma*u\n",
    "    Z = XZ[:,1:]\n",
    "\n",
    "    # Calculate y\n",
    "    y = X*beta + u\n",
    "\n",
    "    return y,X,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gj(b,y,X,Z):\n",
    "    \"\"\"Observations of g_j(b).\n",
    "\n",
    "    This defines the deviations from the predictions of our model; i.e.,\n",
    "    e_j = Z_ju_j, where EZ_ju_j=0.\n",
    "\n",
    "    Can replace this function to testimate a different model.\n",
    "    \"\"\"\n",
    "    return Z*(y - X*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gN(b,data):\n",
    "    \"\"\"Averages of g_j(b).\n",
    "\n",
    "    This is generic for data, to be passed to gj.\n",
    "    \"\"\"\n",
    "    e = gj(b,*data)\n",
    "\n",
    "    # Check to see more obs. than moments.\n",
    "    assert e.shape[0] > e.shape[1]\n",
    "    \n",
    "    return e.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Omegahat(b,data):\n",
    "    e = gj(b,*data)\n",
    "\n",
    "    # Recenter! We have Eu=0 under null.\n",
    "    # Important to use this information.\n",
    "    e = e - e.mean(axis=0) \n",
    "    \n",
    "    return e.T@e/e.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(b,W,data):\n",
    "\n",
    "    m = gN(b,data) # Sample moments @ b\n",
    "    N = data[0].shape[0]\n",
    "\n",
    "    return N*m.T@W@m # Scale by sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def two_step_gmm(data):\n",
    "\n",
    "    # First step uses identity weighting matrix\n",
    "    W1 = np.eye(gj(1,*data).shape[1])\n",
    "\n",
    "    b1 = minimize_scalar(lambda b: J(b,W1,data)).x \n",
    "\n",
    "    # Construct 2nd step weighting matrix using\n",
    "    # first step estimate of beta\n",
    "    W2 = inv(Omegahat(b1,data))\n",
    "\n",
    "    return minimize_scalar(lambda b: J(b,W2,data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_updated_gmm(data):\n",
    "    return minimize_scalar(lambda b: J(b,inv(Omegahat(b,data)),data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 10000\n",
    "sensitivity = 0.01\n",
    "def iterated_gmm(data):\n",
    "    \n",
    "    # First step uses identity weighting matrix\n",
    "    Wn = np.eye(gj(1,*data).shape[1])\n",
    "\n",
    "    # Iterate until Wn converges\n",
    "    for n in range(max_iterations):   # restrict number of iterations\n",
    "        bn = minimize_scalar(lambda b: J(b,Wn,data)).x \n",
    "        Wn_new = inv(Omegahat(bn,data))\n",
    "        diff_mat = Wn - Wn_new # calculate difference matrix\n",
    "        max_diff = np.amax(np.absolute(diff_mat)) # find biggest element, in abs value\n",
    "        if max_diff <= sensitivity: # if the biggest difference is small enough, return matrix\n",
    "            return minimize_scalar(lambda b: J(b,Wn_new,data))\n",
    "        else:\n",
    "            Wn = Wn_new # if not, reassign Wn to be the new mat and repeat\n",
    "    \n",
    "    return minimize_scalar(lambda b: J(b,Wn,data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw some data\n",
    "N = 1000\n",
    "data = dgp(N,beta,gamma,sigma_u,VXZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
